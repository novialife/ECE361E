Total training time: 2207.74 seconds
Number of model parameters: 3217226
MobileNetv1(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): Block(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): Block(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): Block(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): Block(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): Block(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): Block(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): Block(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): Block(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (9): Block(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (10): Block(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (11): Block(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (12): Block(
      (conv1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (linear): Linear(in_features=1024, out_features=10, bias=True)
)
Training losses: [1.6811994838592645, 1.2782545510460348, 1.0451703879534435, 0.8696558115732335, 0.7245130450524333, 0.6222761716988995, 0.5309014536840532, 0.4560305169781151, 0.3820243103196249, 0.3312300143148893, 0.29161944318457944, 0.25117353319435776, 0.22060766949525576, 0.1920611071197883, 0.17293151179352378, 0.16304859337027724, 0.15029654273634677, 0.13610682285883846, 0.12282342476117641, 0.12002995880820867, 0.11573790356545421, 0.10078728782808613, 0.09888122505877557, 0.09790641135629029, 0.0923573772573029, 0.08884365616075675, 0.08584114706234248, 0.08030220009672367, 0.07764488868915555, 0.07682942089331729, 0.07398739330413397, 0.0685134090047301, 0.06957967266383226, 0.06631385743060647, 0.056387304533225345, 0.06845449829054877, 0.0632682969588834, 0.05500590406856059, 0.05586799063250575, 0.055280102884916166, 0.058321672682281195, 0.04713442784202907, 0.05208093022672779, 0.05088625380964688, 0.04885840438999464, 0.042355961205623564, 0.04769811242499658, 0.04680048007770534, 0.0466255359964974, 0.0437812386568912, 0.04077336642369533, 0.03990502547606102, 0.04317429521814217, 0.03861368108062012, 0.03767037387613369, 0.04026841978028017, 0.03383563609396005, 0.03991045336396245, 0.036377058605320485, 0.03677846324782523, 0.03520079342829411, 0.03794750971916129, 0.03292068075555403, 0.02931507503407795, 0.031657374120386474, 0.03258646805794514, 0.028015142515994837, 0.03537889738933986, 0.02897419098763467, 0.029588909191570704, 0.029964731979991315, 0.028498375181839956, 0.03253292653214215, 0.026842036618269942, 0.025390585010890348, 0.028944213649846347, 0.025585380193326013, 0.028878465388029757, 0.023123511584098106, 0.02580513508753408, 0.02829218207432143, 0.024438634061950074, 0.023962892961296518, 0.02655209368251114, 0.022233050101308053, 0.024780517673867818, 0.024518588288089314, 0.025309542281097015, 0.017742727981562324, 0.02051231377940301, 0.02641271561985035, 0.02306432104842199, 0.02020211784836605, 0.019027081385786022, 0.022569071930384172, 0.017396343808533037, 0.025029346774108327, 0.021871896825827266, 0.018032773581269146, 0.02043687715701273]
Training accuracies: [37.78, 53.718, 62.792, 69.29, 74.354, 78.202, 81.356, 83.942, 86.726, 88.488, 89.778, 91.276, 92.382, 93.352, 93.916, 94.28, 94.704, 95.168, 95.706, 95.774, 95.89, 96.52, 96.55, 96.624, 96.846, 96.92, 97.064, 97.306, 97.318, 97.304, 97.466, 97.706, 97.634, 97.698, 98.042, 97.624, 97.832, 98.084, 98.09, 98.09, 97.946, 98.382, 98.22, 98.28, 98.326, 98.524, 98.398, 98.478, 98.378, 98.472, 98.624, 98.646, 98.582, 98.65, 98.768, 98.648, 98.824, 98.732, 98.784, 98.734, 98.76, 98.722, 98.868, 98.986, 98.89, 98.906, 99.046, 98.706, 99.038, 98.976, 98.996, 99.038, 98.86, 99.12, 99.126, 99.012, 99.126, 99.05, 99.234, 99.12, 99.056, 99.178, 99.212, 99.082, 99.24, 99.172, 99.164, 99.13, 99.408, 99.278, 99.072, 99.19, 99.262, 99.354, 99.254, 99.404, 99.176, 99.262, 99.38, 99.28]
Testing losses: [1.445401119280465, 1.1864664931840534, 1.0591457237171222, 0.934573120708707, 0.8965348095833501, 0.9137748804273484, 0.8763946845561643, 0.885574039779132, 0.8810177007807961, 0.9063832669318477, 0.9403161338613003, 0.9835710163357891, 1.0038331965856915, 1.0282552423356455, 1.005336452134048, 1.0778933567336844, 1.0286221315589132, 1.0617956881281696, 1.1117631227155276, 1.1593845946879326, 1.1285715495483786, 1.1547580021846144, 1.1218922688991209, 1.1209113303619096, 1.1380421496644806, 1.1252607491951954, 1.1585178450693059, 1.143408096289333, 1.1890977115570744, 1.1508507815342914, 1.151751070082942, 1.1594468619250045, 1.1845053034492685, 1.132294302499747, 1.2186608503136453, 1.213594264621976, 1.2038035528569282, 1.303561356248735, 1.2622634594953512, 1.1837778559213952, 1.1622351717345323, 1.2372493615633324, 1.1656249082541164, 1.1705860910536368, 1.1684716736214071, 1.2869766535638254, 1.2206976149655595, 1.2670978601974776, 1.2368328171440317, 1.2398376513885547, 1.2441708234292042, 1.287411019771914, 1.2292714224586003, 1.251982872999167, 1.2413521899452693, 1.2174633786648135, 1.332626230731795, 1.1788227146939387, 1.2700526676600492, 1.286685586730136, 1.2370207588883895, 1.2109478961063336, 1.2822872498367406, 1.2772447647927683, 1.2823279107673258, 1.2675683837902696, 1.3287268296072754, 1.2339854172513456, 1.3050977097281926, 1.318316651296012, 1.2623388563530356, 1.2716132523138313, 1.2253851898108856, 1.242744898494286, 1.3264874597138996, 1.2398979218700263, 1.3701769528509695, 1.2368646007549913, 1.299962762035901, 1.3400861009766785, 1.3168963088264949, 1.3161966189553467, 1.3350497182411483, 1.2681380970568596, 1.3395722523520264, 1.3179304471498803, 1.2727049898497667, 1.2330731319475778, 1.3517405066309096, 1.3357537041736554, 1.3038524245914025, 1.268694722954231, 1.320457472831388, 1.3185045394716384, 1.3180247982846032, 1.433764641043506, 1.2957056074202815, 1.3188214490685282, 1.342244011691854, 1.3794699779039696]
Testing accuracies: [47.86, 57.11, 63.2, 67.52, 68.76, 68.71, 70.83, 71.65, 72.01, 72.78, 72.39, 72.89, 72.5, 72.44, 72.82, 73.64, 73.91, 73.56, 73.85, 74.04, 74.55, 74.05, 74.92, 74.34, 74.8, 75.74, 75.3, 75.28, 75.11, 75.64, 76.01, 75.52, 75.07, 76.03, 75.94, 75.35, 76.23, 74.83, 75.55, 76.14, 76.18, 75.67, 76.57, 76.49, 76.9, 76.04, 76.02, 75.93, 76.11, 76.51, 76.65, 77.2, 77.19, 77.14, 76.45, 76.26, 76.64, 77.15, 77.16, 77.01, 76.39, 76.9, 76.69, 76.55, 76.94, 76.55, 76.94, 77.49, 76.99, 76.42, 77.71, 76.92, 77.7, 77.88, 77.45, 77.27, 77.09, 78.13, 77.62, 77.41, 77.41, 77.38, 77.32, 77.27, 77.28, 77.38, 77.6, 77.99, 78.06, 77.22, 77.81, 78.23, 78.11, 77.98, 77.75, 77.76, 77.63, 77.82, 77.87, 77.62]
